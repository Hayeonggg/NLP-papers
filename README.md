# NLP-papers
NLP 논문을 집중적으로 읽고 리뷰합니다.
(스터디 자료 백업용)
</br></br>


## Paper List
- #### 01 : Distributed Representations of Words and Phrases and their Compositionality
  > [Paper](https://nlp.stanford.edu/pubs/glove.pdf), Presentation</br> 
  > Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. 2014.
  >
  > - Keywords : `GloVe`
  > - Date : 2024.01.11 / 2024.01.25(보완)
  > - Presentor : 유하영
  
- #### 02 : Sequence to Sequence Learning with Neural Networks
  > [Paper](https://proceedings.neurips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf), [Presentation](https://github.com/Hayeonggg/NLP-papers/blob/main/presentations/seq2seq_24.01.18_%EC%9C%A0%ED%95%98%EC%98%81.pdf)</br> 
  > Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. 2014.
  >
  > - Keywords : `seq2seq`
  > - Date : 2024.01.18
  > - Presentor : 유하영

- #### 03 : Sequence to Sequence Learning with Neural Networks
  > [Paper](https://arxiv.org/pdf/1409.0473.pdf), [Presentaion](https://github.com/Hayeonggg/NLP-papers/blob/main/presentations/Attention_24.03.03_%EC%9C%A0%ED%95%98%EC%98%81.pdf)</br> 
  > Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. "Neural machine translation by jointly learning to align and translate." arXiv preprint arXiv:1409.0473 (2014). 
  > - Keywords : `Attention`
  > - Date : 2024.03.03
  > - Presentor : 유하영

- #### 04 : Attention is All you need
  > [Paper]( https://arxiv.org/pdf/1706.03762), [Presentaion](https://github.com/Hayeonggg/NLP-papers/blob/main/presentations/Transformer_240512_%EC%9C%A0%ED%95%98%EC%98%81.pdf)</br> 
  > Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017).
  > - Keywords : `Transformer`
  > - Date : 2024.05.12
  > - Presentor : 유하영


 



